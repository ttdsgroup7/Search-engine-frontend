{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "abstraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os, sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/mnt')\n",
    "nb_path = '/content/notebooks'\n",
    "os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
    "sys.path.insert(0, nb_path)  # or append(nb_path)"
   ],
   "metadata": {
    "id": "bcKC_K7J51nE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "tX9_5ose4mMj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --target=$nb_path pymysql\n",
    "!pip install --target=$nb_path transformers"
   ],
   "metadata": {
    "id": "LQFg3o3C6IjJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pymysql\n",
    "#from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from time import time\n",
    "from collections import defaultdict\n"
   ],
   "metadata": {
    "id": "xjDlWFIIOHo6"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/mnt/MyDrive/test/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTNr1lsq4RP8",
    "outputId": "86aba77d-dd0c-4b6e-d202-9e2b640b38ab"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/mnt/MyDrive/test\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs\n"
   ],
   "metadata": {
    "id": "8s-cNOEE6E1U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/sshleifer/distilbart-cnn-12-6"
   ],
   "metadata": {
    "id": "9HLcaUdl9AEY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "model_name = \"./distilbart-cnn-12-6\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, forced_bos_token_id=0).to('cuda')"
   ],
   "metadata": {
    "id": "beS_IrLrQbkm"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avb1Ph5jd1JJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def connectMysql():\n",
    "    connMysql = pymysql.connect(\n",
    "        host='34.89.114.242',\n",
    "        port=3306,\n",
    "        user='root',\n",
    "        password='!ttds2021',\n",
    "        db='TTDS_group7',\n",
    "        charset='utf8mb4'\n",
    "    )\n",
    "    return connMysql\n",
    "\n",
    "\n",
    "class Abstraction_Generation():\n",
    "    src_text = defaultdict(list)\n",
    "    res = []\n",
    "    part = 0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    def set_text(self, text):\n",
    "        cnt = 0\n",
    "        s = list(text)\n",
    "        for i in s:\n",
    "            if cnt<30:\n",
    "                cnt+=1\n",
    "            else:\n",
    "                self.part += 1\n",
    "                cnt = 1\n",
    "            self.src_text[self.part].append(i.replace('@', ''))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # xsum used to generate one sentence, ideal for title prediction\n",
    "    def Pegasus(self):\n",
    "        model_name = 'google/pegasus-xsum'  # 'google/pegasus-large'\n",
    "        tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        batch = tokenizer(self.src_text, truncation=True, padding='longest', return_tensors=\"pt\").to(self.device)\n",
    "        # model.generate(batch['input_ids'],max_length=...,min_length=...)\n",
    "        # length is sum of token, not words\n",
    "        translated = model.generate(**batch, min_length=30, max_length=100)\n",
    "        return tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "    # generate several sentences, ideal for abstraction\n",
    "\n",
    "    # BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n",
    "    # generate for 380 files fail, load 200 at a time\n",
    "    def Bart(self):\n",
    "        # facebook/bart-base 2.1GB\n",
    "        # distilbart-xsum-12-1 400MB\n",
    "        # https://huggingface.co/sshleifer/distilbart-cnn-12-6 speed\n",
    "        model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "        tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "        # forced_bos_token_id =0 disable support for multilingual models\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_name, forced_bos_token_id=0).to(self.device)\n",
    "        for i in range(self.part+1):\n",
    "            print(\"part {} start\".format(i))\n",
    "            batch = tokenizer(self.src_text[i], truncation=True, padding='longest', return_tensors='pt').to(self.device)\n",
    "            translated = model.generate(batch['input_ids'], min_length=50, max_length=100)\n",
    "            # same effect\n",
    "            # print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in translated])\n",
    "            self.res.extend(tokenizer.batch_decode(translated, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n",
    "        return self.res\n",
    "\n",
    "    def fill_mask(self):\n",
    "        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\n",
    "        tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "        example_english_phrase = \"My friends are <mask> but they eat too many carbs.\"\n",
    "        batch = tok(example_english_phrase, return_tensors='pt')['input_ids']\n",
    "        logits = model(batch).logits\n",
    "        # find index\n",
    "        mask = (batch[0] == tok.mask_token_id).nonzero().item()\n",
    "        probs = logits[0, mask].softmax(dim=0)\n",
    "        values, predictions = probs.topk(5)\n",
    "        print(tok.decode(predictions).split())\n",
    "        # batch = tok(example_english_phrase, return_tensors='pt')\n",
    "        # generated_ids = model.generate(batch['input_ids'])\n",
    "        # return tok.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':        \n",
    "    conn =connectMysql()\n",
    "    cursor = conn.cursor()\n",
    "    sentence = 'select docid,content from to_push'\n",
    "    cursor.execute(sentence)\n",
    "    text = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    text = dict(text)\n",
    "    test = Abstraction_Generation()\n",
    "    test.set_text(text.values())\n",
    "    res = test.Bart()\n",
    "\n",
    "    id = list(text.keys())\n",
    "    sentence = 'update to_push set abstract=(%s) where docid = (%s)'\n",
    "    commitlist = []\n",
    "    for index in range(len(text)):\n",
    "        commitlist.append((res[index],id[index]))\n",
    "\n",
    "    conn = connectMysql()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany(sentence,commitlist)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    # for i in text.keys():\n",
    "    #     text[i] =\n",
    "    # print(test.Pegasus())\n",
    "    # print(test.fill_mask())\n",
    "    # conn.commit()\n",
    "    # conn.close()\n",
    "\n",
    "    # from transformers import pipeline\n",
    "    # print(time())\n",
    "    # summarizer = pipeline('summarization')\n",
    "    # print(summarizer(t,min_length=30,max_length=100))\n",
    "    # print(time())\n"
   ]
  }
 ]
}